{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "052f2015",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: mlxtend in /opt/conda/lib/python3.9/site-packages (0.21.0)\n",
      "Requirement already satisfied: scipy>=1.2.1 in /opt/conda/lib/python3.9/site-packages (from mlxtend) (1.8.1)\n",
      "Requirement already satisfied: numpy>=1.16.2 in /opt/conda/lib/python3.9/site-packages (from mlxtend) (1.22.4)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.9/site-packages (from mlxtend) (63.4.2)\n",
      "Requirement already satisfied: joblib>=0.13.2 in /opt/conda/lib/python3.9/site-packages (from mlxtend) (1.1.0)\n",
      "Requirement already satisfied: scikit-learn>=1.0.2 in /opt/conda/lib/python3.9/site-packages (from mlxtend) (1.1.2)\n",
      "Requirement already satisfied: matplotlib>=3.0.0 in /opt/conda/lib/python3.9/site-packages (from mlxtend) (3.5.2)\n",
      "Requirement already satisfied: pandas>=0.24.2 in /opt/conda/lib/python3.9/site-packages (from mlxtend) (1.4.3)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.9/site-packages (from matplotlib>=3.0.0->mlxtend) (0.11.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.9/site-packages (from matplotlib>=3.0.0->mlxtend) (21.3)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /opt/conda/lib/python3.9/site-packages (from matplotlib>=3.0.0->mlxtend) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.9/site-packages (from matplotlib>=3.0.0->mlxtend) (2.8.2)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.9/site-packages (from matplotlib>=3.0.0->mlxtend) (4.34.4)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.9/site-packages (from matplotlib>=3.0.0->mlxtend) (1.4.4)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.9/site-packages (from matplotlib>=3.0.0->mlxtend) (9.2.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.9/site-packages (from pandas>=0.24.2->mlxtend) (2022.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.9/site-packages (from scikit-learn>=1.0.2->mlxtend) (3.1.0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.9/site-packages (from python-dateutil>=2.7->matplotlib>=3.0.0->mlxtend) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('always')\n",
    "warnings.filterwarnings('ignore')\n",
    "!pip install mlxtend\n",
    "# data visualisation and manipulation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import style\n",
    "import seaborn as sns\n",
    "import os\n",
    "#configure\n",
    "# sets matplotlib to inline and displays graphs below the corressponding cell.\n",
    "%matplotlib inline\n",
    "style.use('fivethirtyeight')\n",
    "sns.set(style='whitegrid',color_codes=True)\n",
    "\n",
    "#nltk\n",
    "import nltk\n",
    "\n",
    "#preprocessing\n",
    "from nltk.corpus import stopwords  #stopwords\n",
    "from nltk import word_tokenize,sent_tokenize # tokenizing\n",
    "from nltk.stem import PorterStemmer,LancasterStemmer  # using the Porter Stemmer and Lancaster Stemmer and others\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer  # lammatizer from WordNet\n",
    "\n",
    "# for part-of-speech tagging\n",
    "from nltk import pos_tag\n",
    "\n",
    "# for named entity recognition (NER)\n",
    "from nltk import ne_chunk\n",
    "\n",
    "# vectorizers for creating the document-term-matrix (DTM)\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\n",
    "\n",
    "# BeautifulSoup libraray\n",
    "from bs4 import BeautifulSoup \n",
    "\n",
    "import re # regex\n",
    "\n",
    "#model_selection\n",
    "from sklearn.model_selection import train_test_split,cross_validate\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "#evaluation\n",
    "from sklearn.metrics import accuracy_score,roc_auc_score \n",
    "from sklearn.metrics import classification_report\n",
    "from mlxtend.plotting import plot_confusion_matrix\n",
    "\n",
    "#preprocessing scikit\n",
    "from sklearn.preprocessing import MinMaxScaler,StandardScaler,LabelEncoder\n",
    "\n",
    "#classifiaction.\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC,SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier,GradientBoostingClassifier,AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB,MultinomialNB\n",
    " \n",
    "#stop-words\n",
    "stop_words=set(nltk.corpus.stopwords.words('english'))\n",
    "\n",
    "#keras\n",
    "import keras\n",
    "from keras.preprocessing.text import one_hot,Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense , Flatten ,Embedding,Input,CuDNNLSTM,LSTM\n",
    "from keras.models import Model\n",
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "\n",
    "#gensim w2v\n",
    "#word2vec\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "import keras\n",
    "from sklearn.model_selection import train_test_split,cross_validate\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense , Flatten ,Embedding,Input,CuDNNLSTM,SpatialDropout1D,Bidirectional,LSTM\n",
    "from keras.models import Model\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from keras.preprocessing.text import one_hot,Tokenizer\n",
    "from keras.initializers import Constant\n",
    "from keras.layers import ReLU\n",
    "from keras.layers import Dropout\n",
    "from keras.models import Sequential\n",
    "\n",
    "from tensorflow import keras\n",
    "from keras import optimizers\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.metrics import f1_score, classification_report, log_loss\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df8d7222",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "013a1838",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8334, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>sarcastic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I just wanna have the same love for Phil Famil...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Can't put into how am that is back</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>colleges making fall announcments, remember yo...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>im ollie im 5'8 and i think my 5'7 boyfriend i...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Birds are not real and you cannot tell me othe...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               tweet  sarcastic\n",
       "0  I just wanna have the same love for Phil Famil...          0\n",
       "1                 Can't put into how am that is back          0\n",
       "2  colleges making fall announcments, remember yo...          0\n",
       "3  im ollie im 5'8 and i think my 5'7 boyfriend i...          1\n",
       "4  Birds are not real and you cannot tell me othe...          1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv('Train_Dataset.csv',index_col=False)\n",
    "test = pd.read_csv('Test_Dataset.csv',index_col=False)\n",
    "    \n",
    "df=pd.concat([train,test])\n",
    "# X_train, y_train = train[\"tweet\"], train[\"sarcastic\"]\n",
    "# X_test, y_test = test[\"tweet\"], test[\"sarcastic\"]\n",
    "# shuffling rows\n",
    "df = df.sample(frac=1).reset_index(drop=True)\n",
    "print(df.shape) \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "979db94b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['tweet'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "29021799",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['sarcastic'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8e31a427",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8334, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>sarcastic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I just wanna have the same love for Phil Famil...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Can't put into how am that is back</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>colleges making fall announcments, remember yo...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>im ollie im 5'8 and i think my 5'7 boyfriend i...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Birds are not real and you cannot tell me othe...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               tweet  sarcastic\n",
       "0  I just wanna have the same love for Phil Famil...          0\n",
       "1                 Can't put into how am that is back          0\n",
       "2  colleges making fall announcments, remember yo...          0\n",
       "3  im ollie im 5'8 and i think my 5'7 boyfriend i...          1\n",
       "4  Birds are not real and you cannot tell me othe...          1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b76316b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to clean and pre-process the text.\n",
    "from bs4 import BeautifulSoup\n",
    "def clean_tweets(review):  \n",
    "    \n",
    "    # 1. Removing html tags\n",
    "#     review_text = BeautifulSoup(review,\"html5lib\").get_text()\n",
    "    \n",
    "    # 2. Retaining only alphabets.\n",
    "    review_text = re.sub(\"[^a-zA-Z]\",\" \",review)\n",
    "    \n",
    "    # 3. Converting to lower case and splitting\n",
    "    word_tokens= review_text.lower().split()\n",
    "    \n",
    "    # 4. Remove stopwords\n",
    "    le=WordNetLemmatizer()\n",
    "    stop_words= set(stopwords.words(\"english\"))     \n",
    "    word_tokens= [le.lemmatize(w) for w in word_tokens if not w in stop_words]\n",
    "    \n",
    "    cleaned_review=\" \".join(word_tokens)\n",
    "    return cleaned_review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6eff7152",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "# # load Google's pre-trained Word2Vec model.\n",
    "pre_w2v_model = gensim.models.KeyedVectors.load_word2vec_format(r'GoogleNews-vectors-negative300.bin', binary=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fde3f941",
   "metadata": {},
   "source": [
    "## if required to to train word2vec on our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "46f91d9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12368\n",
      "12368\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import re\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "sentences=[]\n",
    "sum=0\n",
    "for review in df['tweet']:\n",
    "    sents=tokenizer.tokenize(review.strip())\n",
    "    sum+=len(sents)\n",
    "    for sent in sents:\n",
    "        cleaned_sent=clean_tweets(sent)\n",
    "        sentences.append(cleaned_sent.split()) # can use word_tokenize also.\n",
    "print(sum)\n",
    "print(len(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "058ed6f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total number of words are :  3000000\n"
     ]
    }
   ],
   "source": [
    "# total numberof extracted words.\n",
    "vocab=pre_w2v_model.key_to_index\n",
    "print(\"The total number of words are : \",len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8cd482b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(692617, 721780)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model=gensim.models.Word2Vec(sentences=sentences,vector_size=300,window=10,min_count=1)\n",
    "w2v_model.train(sentences,epochs=10,total_examples=len(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dd444cc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.49477348e-01,  6.25580549e-01,  2.05036968e-01,  2.90152878e-01,\n",
       "        9.40137282e-02, -5.33790469e-01,  4.45721686e-01,  1.05016458e+00,\n",
       "        4.05394472e-02,  2.13565417e-02, -4.84797917e-02, -5.52406430e-01,\n",
       "        3.02482843e-02,  1.86381146e-01, -4.07398552e-01, -3.23596001e-01,\n",
       "        6.97004646e-02,  8.45826492e-02,  1.96088731e-01, -1.26166552e-01,\n",
       "       -5.24816692e-01, -5.46412952e-02,  3.62360507e-01,  6.29515052e-02,\n",
       "        5.62209606e-01,  1.16894394e-01, -5.14419675e-01, -1.06687501e-01,\n",
       "       -2.83807576e-01, -5.89534402e-01,  4.29628417e-02, -2.05852628e-01,\n",
       "       -9.81995985e-02, -2.52909530e-02, -5.89904487e-02,  1.80491097e-02,\n",
       "        1.97870076e-01, -6.28925383e-01,  2.61783116e-02, -2.25632131e-01,\n",
       "       -3.50563169e-01,  5.72916679e-02,  9.62296501e-03, -2.15953887e-01,\n",
       "        1.35499611e-01,  4.53073472e-01,  1.26402259e-01,  2.36439258e-01,\n",
       "       -2.08284199e-01,  4.16210681e-01,  3.67933437e-02, -4.37828414e-02,\n",
       "       -3.15222979e-01,  1.38558269e-01, -4.62994464e-02,  6.02347493e-01,\n",
       "        3.17088157e-01, -8.24202299e-02,  1.12654731e-01,  1.31949857e-01,\n",
       "       -7.13178590e-02, -3.63859504e-01, -1.19131275e-01,  1.98624864e-01,\n",
       "        6.82218224e-02,  2.53180683e-01,  9.85468328e-02,  2.65309393e-01,\n",
       "       -3.57972443e-01,  1.61696836e-01, -7.28798807e-02,  1.70854509e-01,\n",
       "        6.03760302e-01, -5.54735482e-01,  2.23758966e-02,  4.87478375e-01,\n",
       "       -4.88539129e-01, -9.06649753e-02, -2.64260381e-01,  4.96295750e-01,\n",
       "       -7.64189139e-02, -5.93386471e-01,  1.02545649e-01,  7.87422657e-01,\n",
       "        1.80571005e-01,  2.60120422e-01, -1.81118503e-01, -9.77937430e-02,\n",
       "        4.74927992e-01,  4.00254846e-01,  6.61353469e-01, -1.85884744e-01,\n",
       "        4.27206397e-01,  3.73299345e-02,  2.25692406e-01,  4.30406988e-01,\n",
       "        5.50264597e-01, -2.63947189e-01, -3.89134109e-01,  4.31409359e-01,\n",
       "       -5.97797297e-02, -1.19421773e-01,  5.28676271e-01,  1.19375654e-01,\n",
       "        1.39601335e-01, -2.70538121e-01, -3.98155525e-02,  2.46785149e-01,\n",
       "       -5.29083848e-01,  6.76484406e-02, -6.18127882e-01, -3.94425124e-01,\n",
       "       -1.21006437e-01,  4.27229553e-01,  2.58848190e-01,  2.28353739e-01,\n",
       "       -1.28150225e-01, -9.31249037e-02,  5.93043745e-01, -4.78666574e-01,\n",
       "        3.36479336e-01,  4.64498103e-01,  2.17231825e-01, -1.45602331e-01,\n",
       "       -1.29127696e-01,  2.97073811e-01,  1.51689664e-01, -5.53390265e-01,\n",
       "       -1.40721753e-01,  2.75654227e-01,  2.82717884e-01,  3.91639978e-01,\n",
       "        1.68246731e-01, -4.77468222e-01,  1.29263803e-01,  2.54478514e-01,\n",
       "       -4.48978767e-02, -2.54664093e-01, -5.20932078e-01, -6.34358704e-01,\n",
       "        1.38814211e-01, -4.98146027e-01,  2.31507182e-01,  4.70116995e-02,\n",
       "        2.94122756e-01, -1.42129255e-03, -7.15978026e-01, -1.51711196e-01,\n",
       "        3.41784716e-01, -1.61669865e-01,  1.49560630e-01, -1.10413218e+00,\n",
       "       -5.28230608e-01, -2.17755169e-01, -5.25162730e-04,  2.60821402e-01,\n",
       "       -2.80514836e-01, -3.07643265e-01, -1.65766701e-01,  5.16611695e-01,\n",
       "        6.21124282e-02,  5.12464821e-01, -6.85091257e-01,  5.06170690e-01,\n",
       "       -8.43832940e-02,  2.47647390e-01,  1.22027412e-01, -7.79252574e-02,\n",
       "        1.13137349e-01,  9.25792158e-01, -1.69990268e-02, -8.24633539e-02,\n",
       "        4.43133444e-01,  1.77441649e-02, -2.65598029e-01, -1.14231616e-01,\n",
       "        9.57300290e-02, -5.31821787e-01, -1.58140138e-01, -8.89829621e-02,\n",
       "       -1.75837055e-01,  2.73725897e-01, -5.76098919e-01, -3.47652912e-01,\n",
       "       -1.87406037e-02,  2.07822070e-01,  5.47133982e-01,  6.07334316e-01,\n",
       "        1.89055398e-01, -5.12362421e-01, -1.96784452e-01, -2.96412371e-02,\n",
       "       -5.67196488e-01,  8.03533420e-02,  2.71280289e-01, -2.53214955e-01,\n",
       "       -8.45201761e-02, -5.08809984e-01,  3.04378897e-01,  8.57962370e-02,\n",
       "       -3.77665758e-01,  8.24617967e-02, -1.44895867e-01, -3.63247991e-01,\n",
       "        1.24467537e-02, -5.38738184e-02,  5.56535088e-03,  1.59306034e-01,\n",
       "        8.28998089e-02, -7.35768974e-02,  2.22369716e-01, -4.60755914e-01,\n",
       "       -1.08437262e-01,  2.49170866e-02,  3.05927753e-01, -3.46088588e-01,\n",
       "       -1.49896815e-01, -7.83818781e-01, -5.13640046e-01, -4.03222620e-01,\n",
       "        5.55703461e-01,  1.33019343e-01, -3.17797273e-01, -3.48802179e-01,\n",
       "       -1.04339465e-01, -3.43064457e-01,  1.78852484e-01,  1.46760941e-01,\n",
       "       -5.32037079e-01,  1.65957317e-01,  5.11534154e-01, -7.54987448e-03,\n",
       "       -3.52734208e-01,  5.38074970e-01, -3.02306086e-01,  6.04168698e-02,\n",
       "       -1.17232934e-01,  1.45773053e-01,  1.27660558e-01, -6.04471922e-01,\n",
       "        3.18836451e-01, -1.63557470e-01, -4.18268085e-01,  7.20324740e-02,\n",
       "        5.92165887e-02, -3.38717610e-01, -1.26011223e-01,  4.11237255e-02,\n",
       "        1.57443021e-04,  3.30995470e-01,  1.07066765e-01,  3.95836264e-01,\n",
       "        1.49856418e-01, -2.18766287e-01, -4.86968786e-01, -4.87655520e-01,\n",
       "        6.38042986e-01,  3.64225000e-01, -7.89308906e-01, -1.90996066e-01,\n",
       "        2.09360391e-01,  3.91350657e-01, -1.06029049e-01, -6.60835683e-01,\n",
       "       -6.02768660e-01, -7.02757612e-02,  9.95628834e-02,  1.55944675e-01,\n",
       "       -3.42594624e-01,  1.07872404e-01, -2.18240827e-01, -3.52524877e-01,\n",
       "       -9.85063463e-02, -3.41225684e-01,  6.13798201e-01, -4.56018001e-02,\n",
       "        5.58376253e-01,  3.45879607e-02, -4.27267462e-01,  2.94078104e-02,\n",
       "        2.18312651e-01, -1.12155370e-01, -1.15673237e-01,  4.11420137e-01,\n",
       "       -5.22368811e-02, -9.88869276e-03, -6.09816909e-01,  3.40072393e-01,\n",
       "       -6.04883842e-02,  4.65859056e-01, -2.13581156e-02,  4.83380675e-01,\n",
       "        7.04773664e-01,  4.45358157e-02,  3.58298928e-01,  7.16390371e-01,\n",
       "        8.19543004e-02, -2.26110324e-01,  4.89939123e-01,  6.96174577e-02],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv.get_vector('like')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b5f72fff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total number of words are :  3000000\n"
     ]
    }
   ],
   "source": [
    "# total numberof extracted words.\n",
    "vocab=pre_w2v_model.key_to_index\n",
    "print(\"The total number of words are : \",len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a430b6d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('really', 0.575244665145874),\n",
       " ('weird', 0.5676319599151611),\n",
       " ('crazy', 0.5382446050643921),\n",
       " ('kind', 0.5310239195823669),\n",
       " ('maybe', 0.5220045447349548),\n",
       " ('loooove', 0.5187614560127258),\n",
       " ('anymore', 0.5177682042121887),\n",
       " ('Kinda_reminds', 0.5151873230934143),\n",
       " ('definitely', 0.5117844343185425),\n",
       " ('kinda_fishy', 0.5090124607086182)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_w2v_model.most_similar('like')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "041d949b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4638977"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_w2v_model.similarity('good','like')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c913d989",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The no of key-value pairs :  3000000\n"
     ]
    }
   ],
   "source": [
    "word_vec_dict={}\n",
    "for word in vocab:\n",
    "  word_vec_dict[word]=pre_w2v_model.get_vector(word)\n",
    "print(\"The no of key-value pairs : \",len(word_vec_dict)) # should come equal to vocab size\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0ac0123",
   "metadata": {},
   "source": [
    "## Preparing data for keras embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0c099b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tweet']=df['tweet'].apply(clean_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3df6dc28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63\n"
     ]
    }
   ],
   "source": [
    "maxi=-1\n",
    "for i,rev in enumerate(df['tweet']):\n",
    "    tokens=rev.split()\n",
    "    if(len(tokens)>maxi):\n",
    "        maxi=len(tokens)\n",
    "print(maxi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e7bd70b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tok = Tokenizer()\n",
    "tok.fit_on_texts(df['tweet'])\n",
    "vocab_size = len(tok.word_index) + 1\n",
    "encd_rev = tok.texts_to_sequences(df['tweet'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7ebd60f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_rev_len=63 # max lenght of a tweet\n",
    "vocab_size = len(tok.word_index) + 1  # total no of words\n",
    "embed_dim=300 # embedding dimension as choosen in word2vec constructor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "00394401",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8334, 63)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now padding to have a amximum length of 1565\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "pad_rev= pad_sequences(encd_rev, maxlen=max_rev_len, padding='post')\n",
    "pad_rev.shape   # note that we had 100K reviews and we have padded each review to have  a lenght of 1565 words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c8759a3",
   "metadata": {},
   "source": [
    "## creating a w2v embedding matrix for input to embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "30f59712",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now creating the embedding matrix\n",
    "embed_matrix=np.zeros(shape=(vocab_size,embed_dim))\n",
    "for word,i in tok.word_index.items():\n",
    "  embed_vector=word_vec_dict.get(word)\n",
    "  if embed_vector is not None:  # word is in the vocabulary learned by the w2v model\n",
    "    embed_matrix[i]=embed_vector\n",
    "  # if word is not found then embed_vector corressponding to that vector will stay zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "24698b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare train and val sets first\n",
    "\n",
    "Y=keras.utils.to_categorical(df['sarcastic'])  # one hot target as required by NN.\n",
    "x_train,x_test,y_train,y_test=train_test_split(pad_rev,Y,test_size=0.20,random_state=42)\n",
    "\n",
    "# #One Hot Encode Y values:\n",
    "# encoder = LabelEncoder()\n",
    "# y_train = encoder.fit_transform(train['sarcastic'].values)\n",
    "# y_train = to_categorical(y_train) \n",
    "\n",
    "# y_test = encoder.fit_transform(test['sarcastic'].values)\n",
    "# y_test = to_categorical(y_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "49b68ad2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-10 16:55:18.363584: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-01-10 16:55:18.363784: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-01-10 16:55:18.367663: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-01-10 16:55:18.367817: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-01-10 16:55:18.367954: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-01-10 16:55:18.368085: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-01-10 16:55:18.368789: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-01-10 16:55:18.728670: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-01-10 16:55:18.728870: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-01-10 16:55:18.729026: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-01-10 16:55:18.729162: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-01-10 16:55:18.729295: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-01-10 16:55:18.729425: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-01-10 16:55:19.222153: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-01-10 16:55:19.222348: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-01-10 16:55:19.222490: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-01-10 16:55:19.222626: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-01-10 16:55:19.222756: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-01-10 16:55:19.222882: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 22306 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:04:00.0,"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " compute capability: 8.6\n",
      "2023-01-10 16:55:19.223209: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-01-10 16:55:19.223321: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 22290 MB memory:  -> device: 1, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:09:00.0, compute capability: 8.6\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'get_f1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [23]\u001b[0m, in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m#     model.add(Dropout(0.3))\u001b[39;00m\n\u001b[1;32m     10\u001b[0m model\u001b[38;5;241m.\u001b[39madd(Dense(\u001b[38;5;241m2\u001b[39m, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msoftmax\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m---> 11\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcategorical_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m, optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m'\u001b[39m, metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[43mget_f1\u001b[49m])\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# return model\u001b[39;00m\n\u001b[1;32m     13\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39mkeras\u001b[38;5;241m.\u001b[39moptimizers\u001b[38;5;241m.\u001b[39mRMSprop(lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-3\u001b[39m),loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbinary_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m,metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'get_f1' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "\"\"\"\n",
    "Using a Bidiretional LSTM. \n",
    "\"\"\"\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim = vocab_size, output_dim = embed_dim, input_length =max_rev_len))\n",
    "model.add(SpatialDropout1D(0.3))\n",
    "model.add(Bidirectional(LSTM(128, dropout=0.25, recurrent_dropout=0.25)))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "#     model.add(Dropout(0.3))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy',get_f1])\n",
    "# return model\n",
    "model.compile(optimizer=keras.optimizers.RMSprop(lr=1e-3),loss='binary_crossentropy',metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee5f4073",
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify batch size and epocj=hs for training.\n",
    "epochs=10\n",
    "batch_size=64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a9574f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(x_train,y_train,epochs=epochs,batch_size=batch_size,validation_data=(x_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fae8d045",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "predictions = model.predict(x_test, verbose=1)\n",
    "print('Validation Loss:', log_loss(y_test, predictions))\n",
    "print('Test Accuracy', (predictions.argmax(axis = 1) == y_test.argmax(axis = 1)).mean())\n",
    "print('F1 Score:', f1_score(y_test.argmax(axis = 1), predictions.argmax(axis = 1), average='weighted'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d5899e",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = \"checkpoint/cp.ckpt\"\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "\n",
    "# Create a callback that saves the model's weights\n",
    "# cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
    "#                                                  save_weights_only=True,\n",
    "#                                                 verbose=1)\n",
    "\n",
    "\n",
    "cp_callback= [\n",
    "                ModelCheckpoint(filepath=checkpoint_path, monitor= 'val_accuracy', verbose=1, save_best_only=True),\n",
    "                EarlyStopping(patience = 2)\n",
    "            ]\n",
    "# Train the model with the new callback\n",
    "model.fit(x_train, \n",
    "          y_train,  \n",
    "          epochs=epochs,\n",
    "          batch_size=batch_size,\n",
    "          validation_data=(x_test,y_test),\n",
    "          callbacks=cp_callback)  # Pass callback to training\n",
    "\n",
    "# This may generate warnings related to saving the state of the optimizer.\n",
    "# These warnings (and similar warnings throughout this notebook)\n",
    "# are in place to discourage outdated usage, and can be ignored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9157e454",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_f1(y_true, y_pred): #taken from old keras source code\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    f1_val = 2*(precision*recall)/(precision+recall+K.epsilon())\n",
    "    return f1_val\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42406d14",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
