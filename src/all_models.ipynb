{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "def precision_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "def f1_m(y_true, y_pred):\n",
    "    precision = precision_m(y_true, y_pred)\n",
    "    recall = recall_m(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['preserve_unused_tokens=False']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "from absl import flags\n",
    "sys.argv=['preserve_unused_tokens=False']\n",
    "flags.FLAGS(sys.argv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow_hub in /opt/conda/lib/python3.9/site-packages (0.12.0)\n",
      "Requirement already satisfied: protobuf>=3.8.0 in /opt/conda/lib/python3.9/site-packages (from tensorflow_hub) (3.20.1)\n",
      "Requirement already satisfied: numpy>=1.12.0 in /opt/conda/lib/python3.9/site-packages (from tensorflow_hub) (1.22.4)\n",
      "Requirement already satisfied: bert-tensorflow in /opt/conda/lib/python3.9/site-packages (1.0.4)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.9/site-packages (from bert-tensorflow) (1.16.0)\n",
      "Collecting contractions\n",
      "  Using cached contractions-0.1.73-py2.py3-none-any.whl (8.7 kB)\n",
      "Collecting textsearch>=0.0.21\n",
      "  Using cached textsearch-0.0.24-py2.py3-none-any.whl (7.6 kB)\n",
      "Collecting pyahocorasick\n",
      "  Using cached pyahocorasick-1.4.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (109 kB)\n",
      "Collecting anyascii\n",
      "  Using cached anyascii-0.3.1-py3-none-any.whl (287 kB)\n",
      "Installing collected packages: pyahocorasick, anyascii, textsearch, contractions\n",
      "Successfully installed anyascii-0.3.1 contractions-0.1.73 pyahocorasick-1.4.4 textsearch-0.0.24\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow_hub\n",
    "!pip install bert-tensorflow\n",
    "!pip install contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "np.random.seed(1)\n",
    "# tf.set_random_seed(2)\n",
    "tf.random.set_seed(2)\n",
    "\n",
    "import pandas as pd\n",
    "import keras\n",
    "# from tqdm import tqdm\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.metrics import f1_score, classification_report, log_loss\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "# from keras.utils import to_categorical\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, LSTM, Dense, SpatialDropout1D, Bidirectional, Flatten\n",
    "from keras.layers import Dropout, Conv1D, GlobalMaxPool1D, GRU, GlobalAvgPool1D\n",
    "# from keras.optimizers import Adam\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "import contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7500, 2)\n",
      "(834, 2)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "\n",
    "train_path = 'Train_Dataset.csv'\n",
    "test_path = 'Test_Dataset.csv'\n",
    "\n",
    "train = pd.read_csv(train_path)\n",
    "test = pd.read_csv(test_path)\n",
    "\n",
    "data=pd.concat([train,test],axis=0)\n",
    "train, test = train_test_split(data, random_state = 42, test_size=0.1)\n",
    "print(train.shape)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def clean_text(text, mapping):\n",
    "    replace_white_space = [\"\\n\"]\n",
    "    for s in replace_white_space:\n",
    "        text = text.replace(s, \" \")\n",
    "    replace_punctuation = [\"’\", \"‘\", \"´\", \"`\", \"\\'\", r\"\\'\"]\n",
    "    for s in replace_punctuation:\n",
    "        text = text.replace(s, \"'\")\n",
    "    \n",
    "    # Random note: removing the URL's slightly degraded performance, it's possible the model learned that certain URLs were positive/negative\n",
    "    # And was able to extrapolate that to retweets. Could also explain why re-training the Embeddings improves performance.\n",
    "    # remove twitter url's\n",
    "    text = re.sub(r\"http[s]?://t.co/[A-Za-z0-9]*\",\"TWITTERURL\",text)\n",
    "    mapped_string = []\n",
    "    for t in text.split(\" \"):\n",
    "\n",
    "        mapped_string.append(contractions.fix(t))  \n",
    "    return ' '.join(mapped_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training constants\n",
    "MAX_SEQ_LEN = 36 #this is based on a quick analysis of the len of sequences train['text'].apply(lambda x : len(x.split(' '))).quantile(0.95)\n",
    "DEFAULT_BATCH_SIZE = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Tokens: 14570\n",
      "Max Token Index: 14570 \n",
      "\n",
      "Sample Tweet Before Processing: The Vernon in town, best pub going. No questions asked.\n",
      "Sample Tweet After Processing: ['The Vernon in town best pub going No questions asked'] \n",
      "\n",
      "What the model will interpret: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 54, 9508, 9, 1126, 129, 763, 60, 251, 1035, 529]\n"
     ]
    }
   ],
   "source": [
    "# Get tweets from Data frame and convert to list of \"texts\" scrubbing based on clean_text function\n",
    "# CONTRACTION_MAPPING is a map of common contractions(e.g don't => do not)\n",
    "train_text_vec = [clean_text(text, contractions.fix) for text in train['tweet'].values]\n",
    "test_text_vec = [clean_text(text, contractions.fix) for text in test['tweet'].values]\n",
    "\n",
    "\n",
    "# tokenize the sentences\n",
    "tokenizer = Tokenizer(lower=False)\n",
    "tokenizer.fit_on_texts(train_text_vec)\n",
    "train_text_vec = tokenizer.texts_to_sequences(train_text_vec)\n",
    "test_text_vec = tokenizer.texts_to_sequences(test_text_vec)\n",
    "\n",
    "# pad the sequences\n",
    "train_text_vec = pad_sequences(train_text_vec, maxlen=MAX_SEQ_LEN)\n",
    "test_text_vec = pad_sequences(test_text_vec, maxlen=MAX_SEQ_LEN)\n",
    "\n",
    "print('Number of Tokens:', len(tokenizer.word_index))\n",
    "print(\"Max Token Index:\", train_text_vec.max(), \"\\n\")\n",
    "\n",
    "print('Sample Tweet Before Processing:', train[\"tweet\"].values[0])\n",
    "print('Sample Tweet After Processing:', tokenizer.sequences_to_texts([train_text_vec[0]]), '\\n')\n",
    "\n",
    "print('What the model will interpret:', train_text_vec[0].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_text_vec[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One Hot Encode Y values:\n",
    "encoder = LabelEncoder()\n",
    "y_train = encoder.fit_transform(train['sarcastic'].values)\n",
    "y_train = to_categorical(y_train) \n",
    "\n",
    "y_test = encoder.fit_transform(test['sarcastic'].values)\n",
    "y_test = to_categorical(y_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribution of Classes: Counter({0: 5745, 1: 1755})\n",
      "[0.65274151 2.13675214]\n"
     ]
    }
   ],
   "source": [
    "# get an idea of the distribution of the text values\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from collections import Counter\n",
    "ctr = Counter(train['sarcastic'].values)\n",
    "print('Distribution of Classes:', ctr)\n",
    "\n",
    "# get class weights for the training data, this will be used data\n",
    "y_train_int = np.argmax(y_train,axis=1)\n",
    "# cws = class_weight.compute_class_weight('balanced', np.unique(y_train_int), y_train_int)\n",
    "cws=class_weights = compute_class_weight(class_weight = \"balanced\", classes= np.unique(y_train_int), y= y_train_int)\n",
    "print(cws)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BASELINE Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dominant Class:  0\n",
      "Baseline Accuracy Dominant Class 0.7853717026378897\n",
      "F1 Score: 0.6909583146109036\n"
     ]
    }
   ],
   "source": [
    "# \n",
    "print('Dominant Class: ', ctr.most_common(n = 1)[0][0])\n",
    "print('Baseline Accuracy Dominant Class', (ctr.most_common(n = 1)[0][0] == test['sarcastic'].values).mean())\n",
    "\n",
    "preds = np.zeros_like(y_test)\n",
    "preds[:, 0] = 1\n",
    "preds[0] = 1 #done to suppress warning from numpy for f1 score\n",
    "print('F1 Score:', f1_score(y_test, preds, average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline Accuracy Using Naive Bayes:  0.7937649880095923\n",
      "F1 Score: 0.7103767684559419\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Naive Bayse Baseline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "# from sklearn.metrics import plot_confusion_matrix\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "text_clf = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', MultinomialNB()),\n",
    "])\n",
    "text_clf.fit(tokenizer.sequences_to_texts_generator(train_text_vec), y_train.argmax(axis=1))\n",
    "predictions = text_clf.predict(tokenizer.sequences_to_texts_generator(test_text_vec)) \n",
    "print('Baseline Accuracy Using Naive Bayes: ', (predictions == y_test.argmax(axis = 1)).mean())\n",
    "print('F1 Score:', f1_score(y_test.argmax(axis = 1), predictions, average='weighted'))\n",
    "\n",
    "cm = confusion_matrix(y_test.argmax(axis = 1), predictions, labels=encoder.classes_)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm,\n",
    "                              display_labels=encoder.classes_)\n",
    "disp.plot()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# _ = plot_confusion_matrix(y_test.argmax(axis = 1), y_true=predictions,labels=encoder.classes_, normalize=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline Accuracy Using RFC:  0.8237410071942446\n",
      "F1 Score: 0.7762590404150771\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Random Forest Baseline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "text_clf = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', RandomForestClassifier(class_weight='balanced', n_estimators=100)), #100 estimators will be the new default in version 0.22\n",
    "])\n",
    "text_clf.fit(tokenizer.sequences_to_texts_generator(train_text_vec), y_train.argmax(axis=1))\n",
    "predictions = text_clf.predict(tokenizer.sequences_to_texts_generator(test_text_vec)) \n",
    "print('Baseline Accuracy Using RFC: ', (predictions == y_test.argmax(axis = 1)).mean())\n",
    "print('F1 Score:', f1_score(y_test.argmax(axis = 1), predictions, average='weighted'))\n",
    "\n",
    "cm = confusion_matrix(y_test.argmax(axis = 1), predictions, labels=encoder.classes_)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm,\n",
    "                              display_labels=encoder.classes_)\n",
    "disp.plot()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def threshold_search(y_true, y_proba, average = None):\n",
    "    best_threshold = 0\n",
    "    best_score = 0\n",
    "    for threshold in [i * 0.01 for i in range(100)]:\n",
    "        score = f1_score(y_true=y_true, y_pred=y_proba > threshold, average=average)\n",
    "        if score > best_score:\n",
    "            best_threshold = threshold\n",
    "            best_score = score\n",
    "    search_result = {'threshold': best_threshold, 'f1': best_score}\n",
    "    return search_result\n",
    "\n",
    "\n",
    "def train(model, \n",
    "          X_train, y_train, X_test, y_test, \n",
    "          checkpoint_path=r'model.{epoch:02d}-{val_loss:.2f}.h5', \n",
    "          epcohs = 25, \n",
    "          batch_size = DEFAULT_BATCH_SIZE, \n",
    "          class_weights = None, \n",
    "          fit_verbose=2,\n",
    "          print_summary = True\n",
    "         ):\n",
    "    m = model()\n",
    "    if print_summary:\n",
    "        print(m.summary())\n",
    "    with tf.device('/cpu:0'):\n",
    "        m.fit(\n",
    "            X_train, \n",
    "            y_train, \n",
    "            #this is bad practice using test data for validation, in a real case would use a seperate validation set\n",
    "            validation_data=(X_test, y_test),  \n",
    "            epochs=epcohs, \n",
    "            batch_size=batch_size,\n",
    "            # class_weight=class_weights,\n",
    "             #saves the most accurate model, usually you would save the one with the lowest loss\n",
    "            callbacks= [\n",
    "                ModelCheckpoint(filepath=checkpoint_path, monitor='val_accuracy', verbose=1, save_best_only=True),\n",
    "                EarlyStopping(patience = 2)\n",
    "            ],\n",
    "            verbose=fit_verbose\n",
    "        ) \n",
    "    print(\"\\n\\n****************************\\n\\n\")\n",
    "    print('Loading Best Model...')\n",
    "    # m.load_weights(checkpoint_path)\n",
    "    with tf.device('/cpu:0'):\n",
    "        predictions = m.predict(X_test, verbose=1)\n",
    "    print('Validation Loss:', log_loss(y_test, predictions))\n",
    "    print('Test Accuracy', (predictions.argmax(axis = 1) == y_test.argmax(axis = 1)).mean())\n",
    "    print('F1 Score:', f1_score(y_test.argmax(axis = 1), predictions.argmax(axis = 1), average='weighted'))\n",
    "    # cm = confusion_matrix(y_test.argmax(axis = 1), predictions, labels=encoder.classes_)\n",
    "    # disp = ConfusionMatrixDisplay(confusion_matrix=cm,\n",
    "    #                             display_labels=encoder.classes_)\n",
    "    # disp.plot()\n",
    "\n",
    "    # plt.show()\n",
    "    return m #returns best performing model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### deep learning architectures\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-13 01:48:56.339731: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-01-13 01:48:56.340050: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-01-13 01:48:56.346600: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-01-13 01:48:56.346867: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-01-13 01:48:56.347098: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-01-13 01:48:56.347325: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-01-13 01:48:56.348012: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-01-13 01:48:56.717322: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-01-13 01:48:56.717485: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-01-13 01:48:56.717621: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-01-13 01:48:56.717738: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-01-13 01:48:56.717855: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-01-13 01:48:56.717977: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-01-13 01:48:57.209083: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-01-13 01:48:57.209268: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-01-13 01:48:57.209401: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-01-13 01:48:57.209527: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-01-13 01:48:57.209650: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-01-13 01:48:57.209770: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 22306 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:04:00.0, compute capability: 8.6\n",
      "2023-01-13 01:48:57.210064: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-01-13 01:48:57.210173: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 22290 MB memory:  -> device: 1, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:09:00.0, compute capability: 8.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 36, 128)           1865088   \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 128)               131584    \n",
      "                                                                 \n",
      " dense (Dense)               (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 2)                 130       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,005,058\n",
      "Trainable params: 2,005,058\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/25\n",
      "\n",
      "Epoch 1: val_accuracy improved from -inf to 0.83693, saving model to model.01-0.39.h5\n",
      "469/469 - 8s - loss: 0.5052 - accuracy: 0.7752 - val_loss: 0.3886 - val_accuracy: 0.8369 - 8s/epoch - 17ms/step\n",
      "Epoch 2/25\n",
      "\n",
      "Epoch 2: val_accuracy improved from 0.83693 to 0.90408, saving model to model.02-0.26.h5\n",
      "469/469 - 7s - loss: 0.1931 - accuracy: 0.9288 - val_loss: 0.2619 - val_accuracy: 0.9041 - 7s/epoch - 15ms/step\n",
      "Epoch 3/25\n",
      "\n",
      "Epoch 3: val_accuracy improved from 0.90408 to 0.90887, saving model to model.03-0.29.h5\n",
      "469/469 - 7s - loss: 0.0546 - accuracy: 0.9823 - val_loss: 0.2850 - val_accuracy: 0.9089 - 7s/epoch - 15ms/step\n",
      "Epoch 4/25\n",
      "\n",
      "Epoch 4: val_accuracy did not improve from 0.90887\n",
      "469/469 - 7s - loss: 0.0263 - accuracy: 0.9909 - val_loss: 0.3362 - val_accuracy: 0.9053 - 7s/epoch - 15ms/step\n",
      "\n",
      "\n",
      "****************************\n",
      "\n",
      "\n",
      "Loading Best Model...\n",
      "27/27 [==============================] - 0s 6ms/step\n",
      "Validation Loss: 0.336236468007167\n",
      "Test Accuracy 0.9052757793764988\n",
      "F1 Score: 0.907128455927337\n"
     ]
    }
   ],
   "source": [
    "def model_1():\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim = (len(tokenizer.word_counts) + 1), output_dim = 128, input_length = MAX_SEQ_LEN))\n",
    "    model.add(LSTM(128))\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dense(2, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "m1 = train(model_1, \n",
    "           train_text_vec,\n",
    "           y_train,\n",
    "           test_text_vec,\n",
    "           y_test,\n",
    "           class_weights=cws\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     (None, 36, 128)           1865088   \n",
      "                                                                 \n",
      " spatial_dropout1d (SpatialD  (None, 36, 128)          0         \n",
      " ropout1D)                                                       \n",
      "                                                                 \n",
      " bidirectional (Bidirectiona  (None, 256)              263168    \n",
      " l)                                                              \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 64)                16448     \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 2)                 130       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,144,834\n",
      "Trainable params: 2,144,834\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/25\n",
      "\n",
      "Epoch 1: val_accuracy improved from -inf to 0.81415, saving model to model_1b.h5\n",
      "469/469 - 34s - loss: 0.5219 - accuracy: 0.7687 - val_loss: 0.4309 - val_accuracy: 0.8141 - 34s/epoch - 72ms/step\n",
      "Epoch 2/25\n",
      "\n",
      "Epoch 2: val_accuracy improved from 0.81415 to 0.86331, saving model to model_1b.h5\n",
      "469/469 - 32s - loss: 0.2631 - accuracy: 0.8956 - val_loss: 0.3188 - val_accuracy: 0.8633 - 32s/epoch - 68ms/step\n",
      "Epoch 3/25\n",
      "\n",
      "Epoch 3: val_accuracy improved from 0.86331 to 0.88489, saving model to model_1b.h5\n",
      "469/469 - 32s - loss: 0.0967 - accuracy: 0.9652 - val_loss: 0.2972 - val_accuracy: 0.8849 - 32s/epoch - 68ms/step\n",
      "Epoch 4/25\n",
      "\n",
      "Epoch 4: val_accuracy improved from 0.88489 to 0.89568, saving model to model_1b.h5\n",
      "469/469 - 32s - loss: 0.0484 - accuracy: 0.9836 - val_loss: 0.3274 - val_accuracy: 0.8957 - 32s/epoch - 68ms/step\n",
      "Epoch 5/25\n",
      "\n",
      "Epoch 5: val_accuracy improved from 0.89568 to 0.91007, saving model to model_1b.h5\n",
      "469/469 - 32s - loss: 0.0263 - accuracy: 0.9915 - val_loss: 0.3314 - val_accuracy: 0.9101 - 32s/epoch - 68ms/step\n",
      "\n",
      "\n",
      "****************************\n",
      "\n",
      "\n",
      "Loading Best Model...\n",
      "27/27 [==============================] - 1s 49ms/step\n",
      "Validation Loss: 0.33142681548309\n",
      "Test Accuracy 0.9100719424460432\n",
      "F1 Score: 0.9097947005235455\n"
     ]
    }
   ],
   "source": [
    "def model_1b():\n",
    "    \"\"\"\n",
    "    Using a Bidiretional LSTM. \n",
    "    \"\"\"\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim = (len(tokenizer.word_counts) + 1), output_dim = 128, input_length = MAX_SEQ_LEN))\n",
    "    model.add(SpatialDropout1D(0.3))\n",
    "    model.add(Bidirectional(LSTM(128, dropout=0.25, recurrent_dropout=0.25)))\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "#     model.add(Dropout(0.3))\n",
    "    model.add(Dense(2, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "m2= train(model_1b, \n",
    "           train_text_vec,\n",
    "           y_train,\n",
    "           test_text_vec,\n",
    "           y_test,\n",
    "           checkpoint_path='model_1b.h5',\n",
    "           class_weights=cws,\n",
    "           print_summary = True\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_2 (Embedding)     (None, 36, 128)           1865088   \n",
      "                                                                 \n",
      " spatial_dropout1d_1 (Spatia  (None, 36, 128)          0         \n",
      " lDropout1D)                                                     \n",
      "                                                                 \n",
      " bidirectional_1 (Bidirectio  (None, 36, 256)          263168    \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " conv1d (Conv1D)             (None, 33, 64)            65600     \n",
      "                                                                 \n",
      " global_max_pooling1d (Globa  (None, 64)               0         \n",
      " lMaxPooling1D)                                                  \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 2)                 130       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,198,146\n",
      "Trainable params: 2,198,146\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/25\n",
      "\n",
      "Epoch 1: val_accuracy improved from -inf to 0.81894, saving model to model_1c.h5\n",
      "469/469 - 34s - loss: 0.5215 - accuracy: 0.7687 - val_loss: 0.4065 - val_accuracy: 0.8189 - 34s/epoch - 73ms/step\n",
      "Epoch 2/25\n",
      "\n",
      "Epoch 2: val_accuracy improved from 0.81894 to 0.88609, saving model to model_1c.h5\n",
      "469/469 - 32s - loss: 0.2668 - accuracy: 0.8929 - val_loss: 0.2779 - val_accuracy: 0.8861 - 32s/epoch - 69ms/step\n",
      "Epoch 3/25\n",
      "\n",
      "Epoch 3: val_accuracy improved from 0.88609 to 0.91007, saving model to model_1c.h5\n",
      "469/469 - 32s - loss: 0.0892 - accuracy: 0.9667 - val_loss: 0.2829 - val_accuracy: 0.9101 - 32s/epoch - 69ms/step\n",
      "Epoch 4/25\n",
      "\n",
      "Epoch 4: val_accuracy improved from 0.91007 to 0.92446, saving model to model_1c.h5\n",
      "469/469 - 32s - loss: 0.0397 - accuracy: 0.9871 - val_loss: 0.3170 - val_accuracy: 0.9245 - 32s/epoch - 69ms/step\n",
      "\n",
      "\n",
      "****************************\n",
      "\n",
      "\n",
      "Loading Best Model...\n",
      "27/27 [==============================] - 2s 50ms/step\n",
      "Validation Loss: 0.3170411985217693\n",
      "Test Accuracy 0.9244604316546763\n",
      "F1 Score: 0.9248359647989787\n"
     ]
    }
   ],
   "source": [
    "##  Convolution LSTM\n",
    "\n",
    "def model_1c():\n",
    "    \"\"\"\n",
    "    Adding dropout to reduce overfitting using a bidiretional LSTM\n",
    "    \"\"\"\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim = (len(tokenizer.word_counts) + 1), output_dim = 128, input_length = MAX_SEQ_LEN))\n",
    "    model.add(SpatialDropout1D(0.3))\n",
    "    model.add(Bidirectional(LSTM(128, dropout=0.25, recurrent_dropout=0.25, return_sequences=True)))\n",
    "    model.add(Conv1D(64, 4))\n",
    "#     model.add(Flatten())\n",
    "    model.add(GlobalMaxPool1D())\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dense(2, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "#     print(model.summary())\n",
    "    return model\n",
    "\n",
    "\n",
    "m3 = train(model_1c, \n",
    "           train_text_vec,\n",
    "           y_train,\n",
    "           test_text_vec,\n",
    "           y_test,\n",
    "           checkpoint_path='model_1c.h5',\n",
    "           class_weights=cws,\n",
    "           print_summary = True\n",
    "          )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Only Convolution Netwrok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_3 (Embedding)     (None, 36, 128)           1865088   \n",
      "                                                                 \n",
      " spatial_dropout1d_2 (Spatia  (None, 36, 128)          0         \n",
      " lDropout1D)                                                     \n",
      "                                                                 \n",
      " conv1d_1 (Conv1D)           (None, 32, 64)            41024     \n",
      "                                                                 \n",
      " conv1d_2 (Conv1D)           (None, 30, 64)            12352     \n",
      "                                                                 \n",
      " conv1d_3 (Conv1D)           (None, 29, 64)            8256      \n",
      "                                                                 \n",
      " global_max_pooling1d_1 (Glo  (None, 64)               0         \n",
      " balMaxPooling1D)                                                \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 2)                 130       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,931,010\n",
      "Trainable params: 1,931,010\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/25\n",
      "\n",
      "Epoch 1: val_accuracy improved from -inf to 0.80456, saving model to model_1d.h5\n",
      "469/469 - 2s - loss: 0.5240 - accuracy: 0.7700 - val_loss: 0.4296 - val_accuracy: 0.8046 - 2s/epoch - 4ms/step\n",
      "Epoch 2/25\n",
      "\n",
      "Epoch 2: val_accuracy improved from 0.80456 to 0.87650, saving model to model_1d.h5\n",
      "469/469 - 2s - loss: 0.2709 - accuracy: 0.8921 - val_loss: 0.3194 - val_accuracy: 0.8765 - 2s/epoch - 3ms/step\n",
      "Epoch 3/25\n",
      "\n",
      "Epoch 3: val_accuracy did not improve from 0.87650\n",
      "469/469 - 1s - loss: 0.0834 - accuracy: 0.9717 - val_loss: 0.3409 - val_accuracy: 0.8741 - 1s/epoch - 3ms/step\n",
      "Epoch 4/25\n",
      "\n",
      "Epoch 4: val_accuracy improved from 0.87650 to 0.88849, saving model to model_1d.h5\n",
      "469/469 - 2s - loss: 0.0356 - accuracy: 0.9893 - val_loss: 0.3890 - val_accuracy: 0.8885 - 2s/epoch - 3ms/step\n",
      "\n",
      "\n",
      "****************************\n",
      "\n",
      "\n",
      "Loading Best Model...\n",
      "27/27 [==============================] - 0s 2ms/step\n",
      "Validation Loss: 0.38895544024100315\n",
      "Test Accuracy 0.8884892086330936\n",
      "F1 Score: 0.8866821191817106\n"
     ]
    }
   ],
   "source": [
    "def model_1d():\n",
    "    \"\"\"\n",
    "    Just for fun below is a model only using covolutions. This is pretty good and also trains very quickly(and predictions would also likely be fast) compared to the LSTM...\n",
    "    It's equivalent to using an n-gram based approach.\n",
    "    Usually in practice you would use a more complex architecture with multiple parallel convolutions that are combined before pooling(and usually both max and avg).\n",
    "    Pure Convolutional NLP is definitely a solution worth exploring further.\n",
    "    \"\"\"\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim = (len(tokenizer.word_counts) + 1), output_dim = 128, input_length = MAX_SEQ_LEN))\n",
    "    model.add(SpatialDropout1D(0.3))\n",
    "    model.add(Conv1D(64, 5))\n",
    "    model.add(Conv1D(64, 3))\n",
    "    model.add(Conv1D(64, 2))\n",
    "    model.add(GlobalMaxPool1D())\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dense(2, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n",
    "m4 = train(model_1d, \n",
    "           train_text_vec,\n",
    "           y_train,\n",
    "           test_text_vec,\n",
    "           y_test,\n",
    "           checkpoint_path='model_1d.h5',\n",
    "           class_weights=cws,\n",
    "           print_summary = True\n",
    "          )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transfer learning with Glove embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_coefs(word,*arr): \n",
    "    return word, np.asarray(arr, dtype='float32')\n",
    "\n",
    "def get_embdedings_matrix(embeddings_index, word_index, nb_words = None):\n",
    "    all_embs = np.stack(embeddings_index.values())\n",
    "    print('Shape of Full Embeddding Matrix', all_embs.shape)\n",
    "    embed_dims = all_embs.shape[1]\n",
    "    emb_mean,emb_std = all_embs.mean(), all_embs.std()\n",
    "\n",
    "    #best to free up memory, given the size, which is usually ~3-4GB in memory\n",
    "    del all_embs\n",
    "    if nb_words is None:\n",
    "        nb_words = len(word_index)\n",
    "    else:\n",
    "        nb_words = min(nb_words, len(word_index))\n",
    "    \n",
    "    embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_dims))\n",
    "    found_vectors = 0\n",
    "    words_not_found = []\n",
    "    for word, i in tqdm(word_index.items()):\n",
    "        if i >= nb_words: \n",
    "            continue\n",
    "        embedding_vector = None\n",
    "        if word in embeddings_index:\n",
    "            embedding_vector = embeddings_index.get(word)\n",
    "        elif word.lower() in embeddings_index:\n",
    "            embedding_vector = embeddings_index.get(word.lower())\n",
    "        # for twitter check if the key is a hashtag\n",
    "        elif '#'+word.lower() in embeddings_index:\n",
    "            embedding_vector = embeddings_index.get('#'+word.lower())\n",
    "            \n",
    "        if embedding_vector is not None: \n",
    "            found_vectors += 1\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "        else:\n",
    "            words_not_found.append((word, i))\n",
    "\n",
    "    print(\"% of Vectors found in Corpus\", found_vectors / nb_words)\n",
    "    return embedding_matrix, words_not_found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_glove(word_index):\n",
    "#     print('Loading Glove')\n",
    "    embed_file_path = 'glove.840B.300d.txt'\n",
    "    embeddings_index = dict(get_coefs(*o.split(\" \")) for o in tqdm(open(embed_file_path)))\n",
    "    print(\"Built Embedding Index:\", len(embeddings_index))\n",
    "    return get_embdedings_matrix(embeddings_index, word_index)\n",
    "\n",
    "def load_twitter(word_index):\n",
    "#     print('Loading Twitter')\n",
    "    embed_file_path = 'glove.twitter.27B.200d.txt'\n",
    "    embeddings_index = dict(get_coefs(*o.split(\" \")) for o in tqdm(open(embed_file_path)))\n",
    "    print(\"Built Embedding Index:\", len(embeddings_index))\n",
    "    return get_embdedings_matrix(embeddings_index, word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Glove Model...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'load_glove' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [10]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLoading Glove Model...\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m glove_embed_matrix, words_not_found \u001b[38;5;241m=\u001b[39m  \u001b[43mload_glove\u001b[49m(tokenizer\u001b[38;5;241m.\u001b[39mword_index)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'load_glove' is not defined"
     ]
    }
   ],
   "source": [
    "print('Loading Glove Model...')\n",
    "glove_embed_matrix, words_not_found =  load_glove(tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "ba5b40e26ba0b0826d90587d8eb33dc6a43854aec369ef6d8d6c42232aa4d3e1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
