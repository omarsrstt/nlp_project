{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from os import path\n",
    "from pathlib import Path\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "sns.set()\n",
    "\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.pipeline import Pipeline\n",
    "# from sklearn.metrics import accuracy_score, confusion_matrix, recall_score, precision_score, f1_score\n",
    "# import datetime as dt\n",
    "# from sklearn.model_selection import cross_val_score\n",
    "# from sklearn.model_selection import GridSearchCV\n",
    "# import calendar\n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# from sklearn.metrics.pairwise import cosine_similarity\n",
    "# %matplotlib inline\n",
    "# import time\n",
    "# import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: iSarcasmEval, Path: ..\\data\\iSarcasmEval\\train.csv\n"
     ]
    }
   ],
   "source": [
    "# Dataset Config Files\n",
    "\n",
    "from config import Config\n",
    "# ['Reddit', 'iSarcasm']\n",
    "cfg = Config('iSarcasm')\n",
    "dataset = cfg.dataset\n",
    "file_path = cfg.input_file_path \n",
    "comment_col_name = cfg.comment_col_name\n",
    "label_col_name = cfg.label_col_name\n",
    "print(f\"Dataset: {dataset}, Path: {file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>tweet</th>\n",
       "      <th>sarcastic</th>\n",
       "      <th>rephrase</th>\n",
       "      <th>sarcasm</th>\n",
       "      <th>irony</th>\n",
       "      <th>satire</th>\n",
       "      <th>understatement</th>\n",
       "      <th>overstatement</th>\n",
       "      <th>rhetorical_question</th>\n",
       "      <th>text</th>\n",
       "      <th>text_lemmatized</th>\n",
       "      <th>removed_stop_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>the only thing i got from college is a caffein...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>College is really difficult, expensive, tiring...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['the', 'only', 'thing', 'i', 'got', 'from', '...</td>\n",
       "      <td>['thing', 'got', 'college', 'caffeine', 'addic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>i love it when professors draw a big question ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>I do not like when professors don’t write out ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['i', 'love', 'it', 'when', 'professor', 'draw...</td>\n",
       "      <td>['love', 'professor', 'draw', 'big', 'question...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>remember the hundred emails from companies whe...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>I, at the bare minimum, wish companies actuall...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['remember', 'the', 'hundred', 'email', 'from'...</td>\n",
       "      <td>['remember', 'hundred', 'email', 'company', 'c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>today my poppop told me i was not forced to go...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Today my pop-pop told me I was not \"forced\" to...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['today', 'my', 'poppop', 'told', 'me', 'i', '...</td>\n",
       "      <td>['today', 'poppop', 'told', 'wa', 'forced', 'g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>volphancarol littlewhitty mysticalmanatee i di...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>I would say Ted Cruz is an asshole and doesn’t...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['volphancarol', 'littlewhitty', 'mysticalmana...</td>\n",
       "      <td>['volphancarol', 'littlewhitty', 'mysticalmana...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0.1  Unnamed: 0  \\\n",
       "0             0         0.0   \n",
       "1             1         1.0   \n",
       "2             2         2.0   \n",
       "3             3         3.0   \n",
       "4             4         4.0   \n",
       "\n",
       "                                               tweet  sarcastic  \\\n",
       "0  the only thing i got from college is a caffein...        1.0   \n",
       "1  i love it when professors draw a big question ...        1.0   \n",
       "2  remember the hundred emails from companies whe...        1.0   \n",
       "3  today my poppop told me i was not forced to go...        1.0   \n",
       "4  volphancarol littlewhitty mysticalmanatee i di...        1.0   \n",
       "\n",
       "                                            rephrase  sarcasm  irony  satire  \\\n",
       "0  College is really difficult, expensive, tiring...      0.0    1.0     0.0   \n",
       "1  I do not like when professors don’t write out ...      1.0    0.0     0.0   \n",
       "2  I, at the bare minimum, wish companies actuall...      0.0    1.0     0.0   \n",
       "3  Today my pop-pop told me I was not \"forced\" to...      1.0    0.0     0.0   \n",
       "4  I would say Ted Cruz is an asshole and doesn’t...      1.0    0.0     0.0   \n",
       "\n",
       "   understatement  overstatement  rhetorical_question  text  \\\n",
       "0             0.0            0.0                  0.0   NaN   \n",
       "1             0.0            0.0                  0.0   NaN   \n",
       "2             0.0            0.0                  0.0   NaN   \n",
       "3             0.0            0.0                  0.0   NaN   \n",
       "4             0.0            0.0                  0.0   NaN   \n",
       "\n",
       "                                     text_lemmatized  \\\n",
       "0  ['the', 'only', 'thing', 'i', 'got', 'from', '...   \n",
       "1  ['i', 'love', 'it', 'when', 'professor', 'draw...   \n",
       "2  ['remember', 'the', 'hundred', 'email', 'from'...   \n",
       "3  ['today', 'my', 'poppop', 'told', 'me', 'i', '...   \n",
       "4  ['volphancarol', 'littlewhitty', 'mysticalmana...   \n",
       "\n",
       "                                  removed_stop_words  \n",
       "0  ['thing', 'got', 'college', 'caffeine', 'addic...  \n",
       "1  ['love', 'professor', 'draw', 'big', 'question...  \n",
       "2  ['remember', 'hundred', 'email', 'company', 'c...  \n",
       "3  ['today', 'poppop', 'told', 'wa', 'forced', 'g...  \n",
       "4  ['volphancarol', 'littlewhitty', 'mysticalmana...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check if preprocessed file exists, load if it is available\n",
    "assert Path(cfg.preprocessed_file_path_all_cols).exists, f\"File not found {cfg.preprocessed_file_path_all_cols}\"\n",
    "data = pd.read_csv(cfg.preprocessed_file_path_all_cols)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       ['thing', 'got', 'college', 'caffeine', 'addic...\n",
      "1       ['love', 'professor', 'draw', 'big', 'question...\n",
      "2       ['remember', 'hundred', 'email', 'company', 'c...\n",
      "3       ['today', 'poppop', 'told', 'wa', 'forced', 'g...\n",
      "4       ['volphancarol', 'littlewhitty', 'mysticalmana...\n",
      "                              ...                        \n",
      "3462    ['population', 'spike', 'chicago', '9', 'month...\n",
      "3463    ['youd', 'think', 'second', 'last', 'english',...\n",
      "3464    ['im', 'finally', 'surfacing', 'holiday', 'sco...\n",
      "3465    ['couldnt', 'prouder', 'today', 'well', 'done'...\n",
      "3466    ['overheard', '13', 'year', 'old', 'game', 'fr...\n",
      "Name: removed_stop_words, Length: 3467, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(data['removed_stop_words'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple, Iterable\n",
    "from collections import Counter, defaultdict\n",
    "from pprint import pprint\n",
    "import spacy\n",
    "from spacy.tokens import Token\n",
    "import pickle\n",
    "from ast import literal_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 0 ns\n",
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "%%time     \n",
    "def find_keywords(text: pd.Series, n_keywords: int) -> List[Tuple[Tuple[str], int]]:\n",
    "    '''\n",
    "    Extract the top n most frequent keywords from the text.\n",
    "    Keywords are sequences of adjectives and nouns that end in a noun\n",
    "    \n",
    "    Arguments:\n",
    "        text        pd.Series(list(str))\n",
    "                    Each element is an array of strings\n",
    "        n_keywords  int\n",
    "                    the number of keywords to return\n",
    "\n",
    "    Returns:\n",
    "        keywords    list(tuple(str, int))\n",
    "                    List of keywords and their count, sorted by the count\n",
    "    '''\n",
    "    \n",
    "    keywords = []\n",
    "    keywords_dict = defaultdict(int)\n",
    "    \n",
    "    for words in text:\n",
    "        for word in literal_eval(words):\n",
    "            keywords_dict[word] += 1\n",
    "                \n",
    "    # pprint(keywords_dict)                \n",
    "    # We add the values to the keywords list and return it\n",
    "    top_n = Counter(keywords_dict)\n",
    "    for keyword_str, count in top_n.most_common(n_keywords):\n",
    "        keyword = tuple(keyword_str.split(\" \"))\n",
    "        keywords.append((keyword, count))\n",
    "    \n",
    "    return keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(('im',), 109), (('love',), 86), (('day',), 72), (('like',), 70), (('get',), 62), (('time',), 52), (('wa',), 48), (('one',), 48), (('dont',), 44), (('people',), 42), (('really',), 40), (('would',), 38), (('know',), 37), (('ive',), 33), (('go',), 33)]\n"
     ]
    }
   ],
   "source": [
    "# Extract keywords from sarcastic comments\n",
    "sarcastic_comments = data.loc[data[label_col_name] == 1, \"removed_stop_words\"]\n",
    "keywords = find_keywords(sarcastic_comments, n_keywords=15)\n",
    "print(keywords)\n",
    "\n",
    "# print(sarcastic_comments)\n",
    "# for elem in sarcastic_comments:\n",
    "#     for word in literal_eval(elem):\n",
    "#         print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(('im',), 294), (('wa',), 251), (('like',), 209), (('one',), 179), (('time',), 175), (('get',), 171), (('day',), 158), (('people',), 149), (('love',), 141), (('dont',), 135), (('ive',), 134), (('year',), 119), (('really',), 108), (('thing',), 106), (('think',), 105)]\n"
     ]
    }
   ],
   "source": [
    "# Extract keywords from sarcastic comments\n",
    "non_sarcastic_comments = data.loc[data[label_col_name] == 0, \"removed_stop_words\"]\n",
    "keywords = find_keywords(non_sarcastic_comments, n_keywords=15)\n",
    "print(keywords)\n",
    "\n",
    "# print(sarcastic_comments)\n",
    "# for elem in sarcastic_comments:\n",
    "#     for word in literal_eval(elem):\n",
    "#         print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input sentences: 867\n",
      "880\n"
     ]
    }
   ],
   "source": [
    "# Download the spacy english model before running this code\n",
    "# python -m spacy download en_core_web_sm --user\n",
    "\n",
    "class POSKeywordExtractor:\n",
    "    def __init__(self):\n",
    "        # Set up SpaCy in a more efficient way by disabling what we do not need\n",
    "        # This is the dependency parser (parser) and the named entity recognizer (ner)\n",
    "        self.nlp = spacy.load(\n",
    "            'en_core_web_sm', \n",
    "        ) # disable=['ner', 'parser']\n",
    "        # Add the sentencizer to quickly split our text into sentences\n",
    "        # pipe = self.nlp.create_pipe()\n",
    "        # pipeline = ['parser', 'ner', 'tagger']\n",
    "        # for pipe in pipeline:\n",
    "        #     self.nlp.add_pipe(pipe)\n",
    "        # self.nlp.add_pipe() # 'sentencizer'\n",
    "        # Increase the maximum length of text SpaCy can parse in one go\n",
    "        self.nlp.max_length = 1500000\n",
    "        \n",
    "    def validate_keyphrase(self, candidate: Iterable[Token]) -> Iterable[Token]:\n",
    "        '''\n",
    "        Takes in a list of tokens which are all proper nouns, nouns or adjectives\n",
    "        and returns the longest sequence that ends in a proper noun or noun\n",
    "        \n",
    "        Args:\n",
    "            candidate         -- List of spacy tokens\n",
    "        Returns:\n",
    "            longest_keyphrase -- The longest sequence that ends in a noun\n",
    "                                 or proper noun\n",
    "                                 \n",
    "        Example:\n",
    "            candidate = [neural, networks, massively]\n",
    "            longest_keyphrase = [neural, networks]\n",
    "        '''\n",
    "        # YOUR CODE HERE\n",
    "        noun_indices = [idx for idx, token in enumerate(candidate) \n",
    "                      if str(token.pos_) == \"NOUN\" or str(token.pos_) == \"PROPN\"]\n",
    "        max_index = max(noun_indices) if noun_indices != [] else 0\n",
    "        return candidate[:max_index+1]\n",
    "\n",
    "    def keyword_extractor(self, text: pd.Series, n_keywords: int, remove_stop: bool=False) -> List[Tuple[Tuple[str], int]]:\n",
    "        '''\n",
    "        Extract the top n most frequent keywords from the text.\n",
    "        Keywords are sequences of adjectives and nouns that end in a noun\n",
    "        \n",
    "        Arguments:\n",
    "            text        pd.Series(list(str))\n",
    "                        Each element is a string\n",
    "            n_keywords  int\n",
    "                        the number of keywords to return\n",
    "            remove_stop bool\n",
    "                        removes stop words\n",
    "        \n",
    "        Returns:\n",
    "            keywords    list(tuple(str, int))\n",
    "                        List of keywords and their count, sorted by the count\n",
    "        '''\n",
    "        \n",
    "        keywords = []\n",
    "        keywords_dict = defaultdict(int)\n",
    "        \n",
    "        for words in text:\n",
    "            for word in literal_eval(words):\n",
    "                keywords_dict[word] += 1\n",
    "                    \n",
    "        # pprint(keywords_dict)                \n",
    "        # We add the values to the keywords list and return it\n",
    "        top_n = Counter(keywords_dict)\n",
    "        for keyword_str, count in top_n.most_common(n_keywords):\n",
    "            keyword = tuple(keyword_str.split(\" \"))\n",
    "            keywords.append((keyword, count))\n",
    "        \n",
    "        return keywords\n",
    "\n",
    "    def keywords(self, text: str, n_keywords: int, min_words: int) -> List[Tuple[Tuple[str], int]]:\n",
    "        '''\n",
    "        Extract the top n most frequent keywords from the text.\n",
    "        Keywords are sequences of adjectives and nouns that end in a noun\n",
    "        \n",
    "        Arguments:\n",
    "            text            pd.Series(list(str))\n",
    "                            Each element is a string\n",
    "            n_keywords -- the number of keywords to return\n",
    "            min_words  -- the number of words a potential keyphrase has to include\n",
    "                          if this is set to 2, then only keyphrases consisting of 2+ words are counted\n",
    "        Returns:\n",
    "            keywords   -- List of keywords and their count, sorted by the count\n",
    "        '''\n",
    "        print(f\"Input sentences: {len(text.tolist())}\")\n",
    "        str_text = \".\\n\".join(text.tolist())\n",
    "        doc = self.nlp(str_text)\n",
    "        print(len([1 for sent in doc.sents]))\n",
    "        # for sent in doc.sents:\n",
    "        #     print(sent)\n",
    "        # for idx, token in enumerate(doc.sents):\n",
    "        #     print(idx, token)\n",
    "        # print(doc, doc.sents, doc.text, dir(doc))\n",
    "        # for line in text:\n",
    "        #     print(self.nlp(line))\n",
    "        # comments = [self.nlp(comment) for comment in text]\n",
    "        keywords = []\n",
    "        keywords_dict = defaultdict(int)\n",
    "#         # YOUR CODE HERE\n",
    "        \n",
    "        pos_matches = ['NOUN', 'PROPN', 'ADJ']\n",
    "        \n",
    "        \n",
    "        # Go through each sentence\n",
    "        # for sentence in doc.sents: \n",
    "        # for comment in comments:\n",
    "            \n",
    "        #     candidates = [[]]\n",
    "#             for token in sentence:\n",
    "#                 # Check if the token in NOUN, PROPN or ADJ and add to list of candidates\n",
    "#                 if any(pos in str(token.pos_) for pos in pos_matches):\n",
    "#                     # If yes, append to make nominal candidate\n",
    "#                     candidates[-1].append(token)\n",
    "#                 else:\n",
    "#                     candidates.append([])\n",
    "            \n",
    "#             # Remove all empty candidates\n",
    "# #             candidates = list(filter(None, candidates))\n",
    "#             candidates = [elem for elem in candidates if elem != []]\n",
    "            \n",
    "#             # Check if we have keywords\n",
    "#             if len(candidates)>0:\n",
    "#                 temp_keywords = [self.validate_keyphrase(candidate) for candidate in candidates]  \n",
    "#                 temp_keywords = [elem for elem in temp_keywords if len(elem)>=min_words]\n",
    "                \n",
    "#                 # We the add this to the main keywords list\n",
    "#                 for keyword in temp_keywords:\n",
    "#                     if keyword == []: # Skip exceptions\n",
    "#                         continue\n",
    "#                     str_keyword = \" \".join(list(map(str, keyword)))\n",
    "#                     keywords_dict[str_keyword] += 1\n",
    "                    \n",
    "#         # pprint(keywords_dict)                \n",
    "#         # We add the values to the keywords list and return it\n",
    "#         top_n = Counter(keywords_dict)\n",
    "#         for keyword_str, count in top_n.most_common(n_keywords):\n",
    "#             keyword = tuple(keyword_str.split(\" \"))\n",
    "#             keywords.append((keyword, count))\n",
    "        \n",
    "        # return keywords\n",
    "        return 0\n",
    "\n",
    "\n",
    "# POSKeywordExtractor()\n",
    "sarcastic_comments = data.loc[data[label_col_name] == 1, \"tweet\"]\n",
    "# print(sarcastic_comments)\n",
    "keywords = POSKeywordExtractor().keywords(sarcastic_comments, n_keywords=15, min_words=1)\n",
    "# print(keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/srv/shares/NLP/wiki_nlp.txt', 'r') as corpus_file:\n",
    "    text = corpus_file.read()\n",
    "    \n",
    "keywords = POSKeywordExtractor().keywords(text.lower(), n_keywords=15, min_words=1)\n",
    "\n",
    "'''\n",
    "Expected output:\n",
    "The keyword ('words',) appears 353 times.\n",
    "The keyword ('text',) appears 342 times.\n",
    "The keyword ('example',) appears 263 times.\n",
    "The keyword ('word',) appears 231 times.\n",
    "The keyword ('natural', 'language', 'processing') appears 184 times.\n",
    "...\n",
    "'''\n",
    "for keyword in keywords:\n",
    "    print('The keyword {} appears {} times.'.format(*keyword))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ADJ  --  adjective\n",
      "ADP  --  adposition\n",
      "ADV  --  adverb\n",
      "AUX  --  auxiliary\n",
      "CCONJ  --  coordinating conjunction\n",
      "CONJ  --  conjunction\n",
      "DET  --  determiner\n",
      "EOL  --  end of line\n",
      "IDS  --  None\n",
      "INTJ  --  interjection\n",
      "NAMES  --  None\n",
      "NOUN  --  noun\n",
      "NO_TAG  --  None\n",
      "NUM  --  numeral\n",
      "PART  --  particle\n",
      "PRON  --  pronoun\n",
      "PROPN  --  proper noun\n",
      "PUNCT  --  punctuation\n",
      "SCONJ  --  subordinating conjunction\n",
      "SPACE  --  space\n",
      "SYM  --  symbol\n",
      "VERB  --  verb\n",
      "X  --  other\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\omars\\miniconda3\\envs\\nlp\\lib\\site-packages\\spacy\\glossary.py:19: UserWarning: [W118] Term 'IDS' not found in glossary. It may however be explained in documentation for the corpora used to train the language. Please check `nlp.meta[\"sources\"]` for any relevant links.\n",
      "  warnings.warn(Warnings.W118.format(term=term))\n",
      "c:\\Users\\omars\\miniconda3\\envs\\nlp\\lib\\site-packages\\spacy\\glossary.py:19: UserWarning: [W118] Term 'NAMES' not found in glossary. It may however be explained in documentation for the corpora used to train the language. Please check `nlp.meta[\"sources\"]` for any relevant links.\n",
      "  warnings.warn(Warnings.W118.format(term=term))\n",
      "c:\\Users\\omars\\miniconda3\\envs\\nlp\\lib\\site-packages\\spacy\\glossary.py:19: UserWarning: [W118] Term 'NO_TAG' not found in glossary. It may however be explained in documentation for the corpora used to train the language. Please check `nlp.meta[\"sources\"]` for any relevant links.\n",
      "  warnings.warn(Warnings.W118.format(term=term))\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Load the English language model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Our sample input\n",
    "text = 'SpaCy is capable of    tagging, parsing and annotating text'# It recognizes sentences and stop words.'\n",
    "\n",
    "# Parse the sample input\n",
    "doc = nlp(text)\n",
    "\n",
    "# print(nlp.pipe_labels)\n",
    "# print(dir(spacy.parts_of_speech))\n",
    "\n",
    "POS_TAGS = ['ADJ', 'ADP', 'ADV', 'AUX', 'CCONJ', 'CONJ', 'DET', 'EOL', 'IDS', 'INTJ', 'NAMES', 'NOUN', 'NO_TAG', 'NUM', 'PART', 'PRON', 'PROPN', 'PUNCT', 'SCONJ', 'SPACE', 'SYM', 'VERB', 'X']\n",
    "for pos_tag in POS_TAGS:\n",
    "    #  print(pos_tag, \" -- \", spacy.explain(pos_tag))\n",
    "     print(pos_tag, \" -- \", spacy.glossary.explain(pos_tag))\n",
    "\n",
    "# For every sentence\n",
    "# for sent in doc.sents:\n",
    "#     print(sent)\n",
    "#     for token in sent:\n",
    "#         print(token)\n",
    "    # For every token\n",
    "    # for token in sent:\n",
    "    #     # Print the token itself, the pos tag, \n",
    "    #     # dependency tag and whether spacy thinks this is a stop word\n",
    "    #     print(token, token.pos_, token.dep_, token.is_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e1961965754ed221a7c2fee85a9c41d05a1f66bee8023eccc6a7de6892ab367c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
