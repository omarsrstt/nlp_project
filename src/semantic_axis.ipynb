{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "from typing import List, Tuple, Dict, List\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VectorModel:\n",
    "    \n",
    "    def __init__(self, vector_dict: Dict[str, np.ndarray]):\n",
    "        self.index = dict()\n",
    "        self.reverse_index = dict()\n",
    "        arrays = []\n",
    "        for idx, item in enumerate(vector_dict.items()):\n",
    "            word, vec = item\n",
    "            arrays.append(vec)\n",
    "            self.index[word] = idx\n",
    "            self.reverse_index[idx] = word\n",
    "        self.embeddings = np.stack(arrays)\n",
    "        \n",
    "    def embed(self, word: str) -> np.ndarray:\n",
    "        if word in self.index:\n",
    "            return self.embeddings[self.index[word]]\n",
    "    \n",
    "    def vector_size(self) -> int:\n",
    "        return self.embeddings.shape[1]\n",
    "    \n",
    "    def cosine_similarity(self, vec1: np.ndarray, vec2: np.ndarray) -> float:\n",
    "        return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))\n",
    "\n",
    "    def most_similar(self, word: str, top_n: int=5) -> List[Tuple[str, float]]:\n",
    "        if word not in self.index:\n",
    "            return None\n",
    "        \n",
    "        sims = []\n",
    "        for idx in range(len(self.index)):\n",
    "            sims.append(self.cosine_similarity(self.embed(word), self.embeddings[idx]))\n",
    "        sim = np.array(sims)\n",
    "        top_n += 1\n",
    "        topn_indices = list(np.argpartition(sims, -top_n)[-top_n:])\n",
    "        if self.index[word] in topn_indices:\n",
    "            topn_indices.remove(self.index[word])\n",
    "        else:\n",
    "            topn_indices = topn_indices[:-1]\n",
    "        \n",
    "        result = []\n",
    "        for idx in topn_indices:\n",
    "            result.append((self.reverse_index[idx], sim[idx]))\n",
    "        return sorted(result, key=lambda x: -x[1])\n",
    "        \n",
    "    def most_similar_vec(self, vec: np.ndarray, top_n: int=5) -> List[Tuple[str, float]]:\n",
    "       \n",
    "        sims = []\n",
    "        for idx in range(len(self.index)):\n",
    "            sims.append(self.cosine_similarity(vec, self.embeddings[idx]))\n",
    "        sim = np.array(sims)\n",
    "        topn_indices = list(np.argpartition(sims, -top_n)[-top_n:])\n",
    "        \n",
    "        result = []\n",
    "        for idx in topn_indices:\n",
    "            result.append((self.reverse_index[idx], sim[idx]))\n",
    "        return sorted(result, key=lambda x: -x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SemanticAxis:\n",
    "    \n",
    "    def __init__(self, model: VectorModel):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            embedding_dict (Dict[str, np.ndarray]) - A dictionary of embedding vectors\n",
    "        \"\"\"\n",
    "        self.model = model\n",
    "    \n",
    "    def fit(\n",
    "        self, \n",
    "        positive_seeds: List[str],\n",
    "        negative_seeds: List[str]\n",
    "    ):\n",
    "        positive_centroid = np.array([self.model.embed(seed) for seed in positive_seeds]).sum(axis=0)\n",
    "        negative_centroid = np.array([self.model.embed(seed) for seed in negative_seeds]).sum(axis=0)\n",
    "        \n",
    "        axis = positive_centroid - negative_centroid\n",
    "        \n",
    "        scores = dict()\n",
    "        \n",
    "        for token in self.model.index:\n",
    "            scores[token] = self.model.cosine_similarity(axis, self.model.embed(token))\n",
    "            \n",
    "        # Make sure scores have zero mean and unit variance\n",
    "        mean = np.mean(list(scores.values()))\n",
    "        std = np.std(list(scores.values()))\n",
    "        \n",
    "        \n",
    "        return {\n",
    "            token: (score - mean) / std\n",
    "            for token, score in scores.items()\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentPropagation:\n",
    "    \"\"\"\n",
    "    Sentiment Propagation according to Hamilton et al (2016)\n",
    "    https://doi.org/10.18653/v1/D16-1057\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, embedding_dict: Dict[str, np.ndarray]):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            embedding_dict (Dict[str, np.ndarray]) - A dictionary of embedding vectors\n",
    "        \"\"\"\n",
    "        self.word_index = dict()\n",
    "        self.index_word = dict()\n",
    "        self.embeddings = list()\n",
    "        for idx, item in enumerate(embedding_dict.items()):\n",
    "            token, embedding = item\n",
    "            self.embeddings.append(embedding)\n",
    "            self.word_index[token] = idx\n",
    "        self.embeddings = np.stack(self.embeddings)\n",
    "        self.vocab_size = len(self.word_index)\n",
    "        \n",
    "    def edge_matrix(self, n_neighbors:int=10) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Construct the graph edge matrix where:\n",
    "        E[i, j] = arccos(-(w[i].T @ w[j]) / (norm(w[i]) * norm(w[j])))\n",
    "        \n",
    "        Args:\n",
    "            n_neighbors (int) - The number of neighbors each vertex in the graph has\n",
    "        Returns:\n",
    "            (np.ndarray)      - The edge matrix\n",
    "        \"\"\"\n",
    "        product = self.embeddings @ self.embeddings.T\n",
    "        normed = np.linalg.norm(self.embeddings, axis=1).reshape(-1, 1)\n",
    "        norm_product = normed @ normed.T\n",
    "        arg = (product / norm_product)\n",
    "        np.fill_diagonal(arg, 0)\n",
    "        for idx, row in enumerate(arg):\n",
    "            arg[idx, np.argsort(row)[:-20]] = 0\n",
    "        return np.arccos(-arg.clip(-1, 1))\n",
    "    \n",
    "    def transition_matrix(self, edge_matrix: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Construct the transition matrix T from the edge matrix\n",
    "        \n",
    "        In contrast to the paper we use 1/s here (maybe an error in the paper?)\n",
    "        \n",
    "        Args:\n",
    "            edge_matrix (np.ndarray) - edge_matrix\n",
    "        Returns:\n",
    "            (np.ndarray)             - The transition matrix\n",
    "        \"\"\"\n",
    "        diag = np.diag([1/s if s > 0 else 0 for s in edge_matrix.sum(axis=0)])**(1/2)\n",
    "        return diag @ edge_matrix @ diag\n",
    "    \n",
    "    def run(self, \n",
    "            T: np.ndarray, s: np.ndarray, p: np.ndarray, \n",
    "            beta: float, eps: float, max_iter: int) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Update p until convergence\n",
    "        \n",
    "        Args:\n",
    "            T (np.ndarray) - Transition matrix\n",
    "            s (np.ndarray) - Seed vector\n",
    "            p (np.ndarray) - Word-Sentiment vector\n",
    "            beta (float)   - Local consistency vs. global consistency\n",
    "            eps (float)    - Convergence criterium\n",
    "            max_iter (int) - Convergence criterium\n",
    "        Returns:\n",
    "            p (np.ndarray) - Learned word-sentiment vector\n",
    "        \"\"\"\n",
    "        for i in range(max_iter):\n",
    "            new_p = (beta * T @ p + (1 - beta) * s)\n",
    "            if np.abs(new_p - p).sum() < eps:\n",
    "                break\n",
    "            p = new_p\n",
    "        return p\n",
    "    \n",
    "    def fit(self, \n",
    "              positive_seeds: List[str],\n",
    "              negative_seeds: List[str], \n",
    "              beta: float=0.1, \n",
    "              n_neighbors: int=10, \n",
    "              eps:float=1e-6,\n",
    "              max_iter:int=100) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Learn sentiment scores\n",
    "        \n",
    "        Args:\n",
    "            positive_seeds (List[str]) - A list of positive seed words\n",
    "            negative_seeds (List[str]) - A list of negative seed words\n",
    "            beta (float)               - Local consistency vs. global consistency\n",
    "            n_neighbors (int)          - The number of neighbors each vertex in the graph has\n",
    "            eps (float)                - Convergence criterium\n",
    "            max_iter (int)             - Convergence criterium\n",
    "        Returns:\n",
    "            (Dict[str, float])         - A dictionary containing the learned sentiment scores\n",
    "                                         with zero mean and unit variance\n",
    "        \"\"\"\n",
    "        T = self.transition_matrix(self.edge_matrix(n_neighbors))\n",
    "        \n",
    "        s_positive = np.zeros(self.vocab_size)\n",
    "        s_negative = np.zeros(self.vocab_size)\n",
    "        \n",
    "        s_positive[[self.word_index[seed] for seed in positive_seeds]] = 1 / len(positive_seeds)\n",
    "        s_negative[[self.word_index[seed] for seed in negative_seeds]] = 1 / len(negative_seeds)\n",
    "        \n",
    "        p_positive = self.run(\n",
    "            T, s_positive.reshape(-1, 1), \n",
    "            np.ones((self.vocab_size, 1)) / self.vocab_size, \n",
    "            beta, eps, max_iter\n",
    "        )\n",
    "        p_negative = self.run(\n",
    "            T, s_negative.reshape(-1, 1),\n",
    "            np.ones((self.vocab_size, 1)) / self.vocab_size,\n",
    "            beta, eps, max_iter\n",
    "        )\n",
    "        \n",
    "        score = p_positive / (p_positive + p_negative)\n",
    "        # Make sure scores have zero mean and unit variance\n",
    "        score = score - np.mean(score)\n",
    "        score = score / np.std(score)\n",
    "        \n",
    "        # Construct dictionary\n",
    "        sentiment = dict()\n",
    "        for token, idx in self.word_index.items():\n",
    "            sentiment[token] = score[idx].item()\n",
    "            \n",
    "        return sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6f05ed4a0d706576b6734957b5c5041fc2ebfaf80bba546154d080e6c318f452"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
